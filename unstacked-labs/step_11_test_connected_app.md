# Test the Connected App

## Overview

Now that you've connected your application to the Gemini model, it's time to test the functionality and see your chatbot in action!

## Testing Instructions

### 1. Restart the Application

Within the terminal, terminate the currently running process by pressing:

```
CTRL+C
```

### 2. Start the Application Again

Re-run the command to start the Streamlit application again:

```shell
streamlit run app.py --browser.serverAddress=localhost --server.enableCORS=false --server.enableXsrfProtection=false --server.port 8080
```

### 3. Refresh the Application

Refresh the Streamlit application. If the Streamlit application is still running, you can simply refresh the web preview page in your browser.

### 4. Ask a Question

Now, type a question into the chat input such as the following:

```
What is the best time of year to go to Iceland?
```

### 5. Observe the Response

Press **ENTER**. 

You should see:
1. The application display your message
2. A "Thinking..." spinner
3. A response generated by the Gemini model!

---

## ðŸŽ‰ Success!

If you see a response from the Gemini model, then you have successfully connected your web application to an LLM on Vertex AI! ðŸ™Œ ðŸ¥³

---

## What's Happening?

When you send a message:
1. Your input is captured by the Streamlit interface
2. It's sent to the `call_model` function
3. The function uses the Vertex AI client to send your prompt to the Gemini 2.5 Flash model
4. The model generates a response
5. The response is displayed in the chat interface

---

**Next Steps:** Now that your basic chatbot is working, you'll learn how to customize its behavior using system instructions and parameters to make it better suited for travel assistance.