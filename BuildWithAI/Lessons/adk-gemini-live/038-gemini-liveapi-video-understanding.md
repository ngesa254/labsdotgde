# Lesson 3.8: Video Understanding with Gemini Live API (Prep Lab)

ðŸ““ `kisumu_video_understanding.ipynb`  
**Colab:** https://colab.research.google.com/drive/1uIsAzF5dKUs-6Jdnaa7FGnCJobme6rDA

## Challenge: Video Analysis With Gemini âš¡ðŸŽ¥

Teach your agent to analyze video streams using Gemini 2.5.

## Prerequisites
* Gemini API key set up
* Familiarity with Lesson 3.6 (Live API Fundamentals)

## Concept
Gemini models have always been multimodal, but **video support in 2.5** enables:
* Scene captioning & object detection
* Text extraction from video frames (sticky notes, signs, slides)
* Summarization of long recordings (user studies, talks)
* Searching inside video timelines

## Task

### 1. Upload sample videos (`Pottery.mp4`, `Trailcam.mp4`, etc.)

### 2. Run prompts for **scene descriptions**, **text extraction**, and **structured tables**.

### 3. Try **YouTube video analysis** with custom time offsets (e.g., I/O keynote timestamps).

### 4. Customize preprocessing with FPS and clipping intervals for efficiency.

## Prompts to Try
* "List all animals in this trailcam video with timecodes."
* "Transcribe sticky notes from this video and generate 3 new ideas."
* "Summarize the Google I/O 2025 keynote between 20:50â€“26:10."

## Outcome
You'll have a working pipeline for **video analysis** â€” useful for research, product studies, or event recordings.

## Question
How could **video understanding** extend your agents? Imagine use cases in **education, customer research, or developer events.**